name: Benchmark
on:
  push:
    paths:
      - "**.py"
      - "pyproject.toml"
      - "pdm.lock"
      - ".github/workflows/test.yaml"

jobs:
  # Check PDM lockfile is up to date. If not, update it, push a commit back to the branch, and fail the workflow.
  # The new commit will trigger a new workflow run.
  lock:
    name: Lock dependencies
    runs-on: ubuntu-latest
    concurrency:
      group: lock-${{ github.ref_name }}
      cancel-in-progress: true
    steps:
      - uses: actions/checkout@v3
        with:
          # NOTE: access token is necessary to make the new commit trigger a new workflow run; see
          # https://github.com/marketplace/actions/git-auto-commit#commits-made-by-this-action-do-not-trigger-new-workflow-runs
          token: ${{ secrets.GH_ACCESS_TOKEN }}
      - name: Set up PDM
        uses: pdm-project/setup-pdm@v2
        with:
          python-version: 3.8  # Must match version on `benchmark` job
          enable-pep582: false
      - name: PDM lock
        shell: bash
        run: |
          pdm install --check --dry-run || pdm lock -v
      - name: Commit updated lockfile
        id: auto-commit
        uses: stefanzweifel/git-auto-commit-action@v4
        with:
          commit_message: "Update lockfile"
          file_pattern: "pdm.lock"
      - name: Halt if out of date
        if: steps.auto-commit.outputs.changes_detected == 'true'
        uses: actions/github-script@v3
        with:
          script: |
              core.setFailed('PDM lockfile was out of date. Newly locked file pushed in ${{ steps.auto-commit.outputs.changes_detected.commit_hash }}.')

  benchmark:
    name: Benchmark
    if: "!contains(github.event.head_commit.message, '[skip ci]')"
    runs-on: ubuntu-latest
    needs: [lock]
    strategy:
      max-parallel: 2
      fail-fast: false
      matrix:
        repeat: [1]
    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0
      - name: Add SHORT_SHA
        shell: bash
        run: echo "SHORT_SHA=$(git rev-parse --short HEAD)" >> "$GITHUB_ENV"
      - name: Add MATRIX_ID
        shell: bash
        # Ensure this will uniquely identify each job per matrix; used in cluster name
        run: echo "MATRIX_ID=${{ matrix.repeat }}" >> "$GITHUB_ENV"
      - name: Set up PDM
        uses: pdm-project/setup-pdm@v2
        with:
          python-version: 3.8 # Must match version on `lock` job
          enable-pep582: false
        # TODO cache virtual env, especially between `lock` job
      - name: Install dependencies
        shell: bash
        run: |
          pdm venv create
          eval $(pdm venv activate in-project)
          pdm use $(which python)
          pdm install -dG test
          pdm info
      - name: Run Tests
        shell: bash
        env:
          DASK_COILED__TOKEN: ${{ secrets.COILED_BENCHMARK_BOT_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-2 # this is needed for boto for some reason
          GITHUB_RUN_ID: ${{ github.run_id }}
          GITHUB_RUN_ATTEMPT: ${{ github.run_attempt }}
        run: |
          pdm run pytest -n 8 --dist loadscope --ignore=tests/test_test.py
      - name: Upload benchmark results as artifact
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: results-${{ matrix.repeat }}
          path: results-${{ env.SHORT_SHA }}.json
    outputs:
      short_sha: ${{ steps.short-sha.outputs.short_sha }}

  # Use a separate job for uploading JSON results to S3, so we can ensure only one runs at once.
  upload-results:
    name: Upload results
    runs-on: ubuntu-latest
    needs: ["benchmark"]
    if: always()
    concurrency:
      group: upload-${{ github.sha }}
      cancel-in-progress: false
    steps:
      - name: Install AWS CLI
        # Install AWS CLI when running locally with `act`, since its smaller images don't have AWS installed.
        # On normal GitHub actions, AWS is already installed on the runners.
        if: ${{ env.ACT }}
        shell: bash
        run: |
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip awscliv2.zip
          sudo ./aws/install
          aws --version
      - name: Download results artifacts
        uses: actions/download-artifact@v3
      - name: Combine and upload benchmark results to bucket
        shell: bash
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-2 # this is needed for boto for some reason
        # Download any existing results for this commit, concatenate results from all artifacts onto it, and re-upload.
        # When we switch to sqlite, this will be basically the same.
        run: |
          results=( results-*/results-*.json )
          FILENAME=$(basename "${results[0]}")

          aws s3 cp "s3://snakebench-public/$FILENAME" . || touch $FILENAME
          echo "$(wc -l < $FILENAME) previous results for $FILENAME found"

          cat "${results[@]}" >> "$FILENAME"
          echo "Uploading $(wc -l < $FILENAME) results for $FILENAME"

          aws s3 cp "$FILENAME" s3://snakebench-public/
