name: Benchmark
on: [push] # TODO only match certain branches

jobs:
  benchmark:
    name: Benchmark
    runs-on: ubuntu-latest
    strategy:
      max-parallel: 10
      matrix:
        repeat: [1, 2, 3, 4]
    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0
      - name: Add SHORT_SHA
        shell: bash
        run: echo "SHORT_SHA=$(git rev-parse --short HEAD)" >> "$GITHUB_ENV"
      - name: Add MATRIX_ID
        shell: bash
        # Ensure this will uniquely identify each job per matrix; used in cluster name
        run: echo "MATRIX_ID=${{ matrix.repeat }}" >> "$GITHUB_ENV"
      - name: Set up PDM
        uses: pdm-project/setup-pdm@v2
        with:
          python-version: 3.8 # TODO matrix
          enable-pep582: false
        # TODO pdm lock if necessary, and push commit back to branch with new lockfile
        # TODO cache virtual env
      - name: Install dependencies
        shell: bash
        run: |
          pdm venv create
          eval $(pdm venv activate in-project)
          pdm use $(which python)
          pdm install -dG test
          pdm info
      - name: Run Tests
        shell: bash
        env:
          DASK_COILED__TOKEN: ${{ secrets.COILED_BENCHMARK_BOT_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-2 # this is needed for boto for some reason
          GITHUB_RUN_ID: ${{ github.run_id }}
          GITHUB_RUN_ATTEMPT: ${{ github.run_attempt }}
        run: |
          pdm run pytest -n 10 --dist loadscope
      - name: Upload benchmark results as artifact
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: results-${{ matrix.repeat }}
          path: results-${{ env.SHORT_SHA }}.json
    outputs:
      short_sha: ${{ steps.short-sha.outputs.short_sha }}

  # Use a separate job for uploading JSON results to S3, so we can ensure only one runs at once.
  upload-results:
    name: Upload results
    runs-on: ubuntu-latest
    needs: ["benchmark"]
    if: always()
    concurrency:
      group: upload-${{ github.sha }}
      cancel-in-progress: false
    steps:
      - name: Install AWS CLI
        # Install AWS CLI when running locally with `act`, since its smaller images don't have AWS installed.
        # On normal GitHub actions, AWS is already installed on the runners.
        if: ${{ env.ACT }}
        shell: bash
        run: |
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip awscliv2.zip
          sudo ./aws/install
          aws --version
      - name: Download results artifacts
        uses: actions/download-artifact@v3
      - name: Combine and upload benchmark results to bucket
        shell: bash
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-2 # this is needed for boto for some reason
        # Download any existing results for this commit, concatenate results from all artifacts onto it, and re-upload.
        # When we switch to sqlite, this will be basically the same.
        run: |
          results=( results-*/results-*.json )
          FILENAME=$(basename "${results[0]}")

          aws s3 cp "s3://snakebench-public/$FILENAME" . || touch $FILENAME
          echo "$(wc -l < $FILENAME) previous results for $FILENAME found"

          cat "${results[@]}" >> "$FILENAME"
          echo "Uploading $(wc -l < $FILENAME) results for $FILENAME"

          aws s3 cp "$FILENAME" s3://snakebench-public/
